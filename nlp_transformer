import string
import tensorflow as tf
import numpy as np
import getConfig
import tensorflow.keras.preprocessing.sequence as sequence
import textClassiferModel as model
import time
import os

def cal_ks(y_true, y_prob, n_bins=20):
    percentile = np.linspace(0, 100, n_bins + 1).tolist()
    bins = [np.percentile(y_prob, i) for i in percentile]
    bins[0] = bins[0] - 0.01
    bins[-1] = bins[-1] + 0.01
    binids = np.digitize(y_prob, bins) - 1
    y_1 = sum(y_true == 1)
    y_0 = sum(y_true == 0)
    bin_true = np.bincount(binids, weights=y_true, minlength=len(bins))
    bin_total = np.bincount(binids, minlength=len(bins))
    bin_false = bin_total - bin_true
    true_pdf = bin_true / y_1
    false_pdf = bin_false / y_0
    true_cdf = np.cumsum(true_pdf)
    false_cdf = np.cumsum(false_pdf)
    ks_list = np.abs(true_cdf - false_cdf).tolist()
    ks = max(ks_list)
    return ks, ks_list

gConfig = getConfig.get_config(config_file='config.ini.nlp')
sentence_size = gConfig['sentence_size']
embedding_size = gConfig['embedding_size']
vocab_size = gConfig['vocabulary_size']
model_dir = gConfig['model_dir']

def read_npz(data_file):
    r = np.load(data_file,allow_pickle=True)
    return r['arr_0'],r['arr_1'],r['arr_2'],r['arr_3']#x_train,y_train,x_test,y_test

def pad_sequences(inp):
    out_sequences=sequence.pad_sequences(inp,maxlen=gConfig['sentence_size'],padding='post',value=0)
    return out_sequences

#x_train,y_train,x_test,y_test = read_npz(gConfig['npz_data'])
#x_train = pad_sequences(x_train)
#x_test = pad_sequences(x_test)
#y_train = y_train.astype(int)
#y_test = y_test.astype(int)
#
#dataset_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)).shuffle(gConfig['shuffle_size'])
#dataset_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)).shuffle(gConfig['shuffle_size'])

checkpoint_path = gConfig['model_dir']
ckpt_manager = tf.train.CheckpointManager(model.ckpt,checkpoint_path,max_to_keep=10)

def create_model(best_ckpt=None):
    ckpt=tf.io.gfile.listdir(checkpoint_path)
    if ckpt:
        print('reload model')
        if best_ckpt:
            model.ckpt.restore(checkpoint_path+'ckpt-'+str(best_ckpt))
            print('reload trained model ckpt-'+str(best_ckpt))
            return model
        else:
            model.ckpt.restore(tf.train.latest_checkpoint(checkpoint_path))
            return model
    else:
        return model

def train():
    model = create_model()
    for epoch in range(gConfig['epochs']):
        start = time.time()
        model.train_loss.reset_states()
        model.train_accuracy.reset_states()
        for (batch,(inp,target)) in enumerate(dataset_train.batch(gConfig['batch_size'])):
            start = time.time()
            loss = model.step(inp,target)
            print('train epoch: Epoch {} Batch {} Loss {:.4f} AUC {:.4f} prestep {:.4f}'.format(epoch+1,batch,loss[0],loss[1],(time.time()-start)))

        for (batch,(inp,target)) in enumerate(dataset_test.batch(gConfig['batch_size'])):
            start = time.time()
            loss = model.evaluate(inp,target)
            print('validate epoch:Epoch {} Batch {} Loss {:.4f} AUC {:.4f} prestep {:.4f}'.format(epoch+1,batch,loss[0],loss[1],(time.time()-start)))

        ckpt_save_path=ckpt_manager.save()
        print('保存epoch{}模型在 {}'.format(epoch+1,ckpt_save_path))
        files = ['all_mob2.csv_process_idx']
        for file in files:
            X, Y, IDS = get_data_label_id(gConfig['data_path'] + file)
            X = pad_sequences(X)
            Y = [int(i) for i in Y]
            dataset_pred = tf.data.Dataset.from_tensor_slices((X,Y))
            output = []
            for (batch,(inp,target)) in enumerate(dataset_pred.batch(gConfig['batch_size'])):
                data = model.prediction(inp,target)
                if len(output)==0:
                    output = data
                else:
                    output = np.concatenate((output,data),axis=0)
            print(file,' ',cal_ks(output[:,0],output[:,1])[0])

def get_data_label_id(file):
    x, y ,ids = [], [], []
    with open(file) as f:
        for line in f:
            lines = line.rstrip('\n').split('\t')
            ids.append(lines[0])
            y.append(lines[1])
            x.append(lines[-1].split(','))
    return np.array(x), np.array(y), ids



def prediction():
    model = create_model(best_ckpt=2)
    files = os.listdir(gConfig['data_path'])
    files = [f for f in files if 'mob2' in f]
    print(files)
    for file in files:
        print(file)
        X, Y, IDS = get_data_label_id(gConfig['data_path'] + file)
        X = pad_sequences(X)
        Y = [int(i) for i in Y]
        #dataset_pred = tf.data.Dataset.from_tensor_slices((X,Y))
        dataset_pred = tf.data.Dataset.from_tensor_slices((X,Y))
        output = []
        for (batch,(inp,target)) in enumerate(dataset_pred.batch(gConfig['batch_size'])):
            data = model.prediction(inp,target)
            if len(output)==0:
                output = data
            else:
                output = np.concatenate((output,data),axis=0)
        #output = np.concatenate((IDS,output),axis=1)
        print(file,' ',cal_ks(output[:,0],output[:,1])[0])
        np.savetxt(gConfig['data_path'] + file +'_prediction',output,delimiter=',')

def get_alldata_label(file):
    x1, x2, x3, y = [], [], [], []
    with open(file) as f:
        for line in f:
            lines = line.rstrip('\n').split('\t')
            y.append(lines[1])
            #x1.append(lines[0].split(',')[3:44])
            #x2.append(lines[0].split(',')[44:])
            x3.append(lines[-1].split(','))
    return np.array(x3), np.array(y)#np.array(x1), np.array(x2), np.array(x3), np.array(y)


#if __name__ == "__main__":
if gConfig['mode'] == 'train':
    x_train,y_train = get_alldata_label('data_idx/train_mob12.csv_process_idx')
    x_test,y_test = get_alldata_label('data_idx/test_mob12.csv_process_idx')
    #x_train,y_train,x_test,y_test = read_npz(gConfig['npz_data'])
    x_train = pad_sequences(x_train)
    x_test = pad_sequences(x_test)
    y_train = y_train.astype(int)
    y_test = y_test.astype(int)
    dataset_train = tf.data.Dataset.from_tensor_slices((x_train,y_train)).shuffle(gConfig['shuffle_size'])
    dataset_test = tf.data.Dataset.from_tensor_slices((x_test,y_test)).shuffle(gConfig['shuffle_size'])
    train()
elif gConfig['mode'] == 'prediction':
    prediction()
